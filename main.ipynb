{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990183fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802a208",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK= \"ARC\"\n",
    "MODEL = \"olmo-2-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6760f5ba",
   "metadata": {},
   "source": [
    "## Stage 1: Primary Preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"./data/test_data/{TASK}_test.json\"\n",
    "CUDA_VISIBLE_DEVICES=0 python evaluate2.py --task $TASK --n 100 --model $MODEL --data_dir $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f85e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python judge2.py --task $TASK --model $MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291066ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/test_data/{TASK}_test.json\", \"r\") as f:\n",
    "    original = json.load(f)\n",
    "\n",
    "with open(f\"./results/T2-judged/{MODEL}_{TASK}.json\", \"r\") as f:\n",
    "    reformed = json.load(f)\n",
    "\n",
    "n_correct_1=0\n",
    "for x, (o, r) in enumerate(zip(original, reformed)):\n",
    "    r['answer'] = [a.replace(\"(\", \"\").replace(\")\", \"\") for a in r['answer']]\n",
    "    if o['gold_answer'] == r['answer'][0]:\n",
    "        n_correct_1+=1\n",
    "\n",
    "print(\"Accuracy: \", n_correct_1/(x+1))\n",
    "print(f'{MODEL}_{TASK}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0e047",
   "metadata": {},
   "source": [
    "## Stage 2: Replace the First Pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./results/T2-judged/{MODEL}_{TASK}.json\", \"r\") as f:\n",
    "    outputs = json.load(f)\n",
    "        \n",
    "pattern = r'\\(([A-Z])\\)\\s*([^()]+)'\n",
    "n_skipped = 0\n",
    "n_correct = 0\n",
    "data = []\n",
    "temp = []\n",
    "\n",
    "for x, o in enumerate(outputs):\n",
    "    try:\n",
    "        o['answer'] = [a.replace(\"(\", \"\").replace(\")\", \"\") for a in o['answer']]\n",
    "        answer = o['answer'][-1]\n",
    "        # Step 1: extract all (A)...(D) options\n",
    "        text = o['question']\n",
    "        options = dict(re.findall(pattern, text))\n",
    "        \n",
    "        options[answer] = 'none of the options'\n",
    "        question_part = re.split(pattern, text)[0].strip()\n",
    "    \n",
    "        new_options = [f\"({i}) {v.strip()}\" for i, v in options.items()]\n",
    "        rewritten = question_part + \" \" + \" \".join(new_options)\n",
    "    \n",
    "        o['question'] = rewritten\n",
    "    \n",
    "        data.append(o)\n",
    "    except Exception as e:\n",
    "        # Added print for debugging errors if needed\n",
    "        print(f\"Error at index {x}: {e}\")\n",
    "        n_skipped += 1\n",
    "        temp.append(x)\n",
    "        \n",
    "print(f\"Skipped: {n_skipped}\")\n",
    "\n",
    "os.makedirs(\"./data/removed/\", exist_ok=True)\n",
    "save_path = f\"./data/removed/{MODEL}_{TASK}.json\"\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a67c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"./data/removed/{MODEL}_{TASK}.json\"\n",
    "CUDA_VISIBLE_DEVICES=0 python evaluate2.py --task $TASK --n 100 --model $MODEL --data_dir $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf04a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python judge2.py --task $TASK --model $MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ec5c5",
   "metadata": {},
   "source": [
    "## Stage 3: IoT Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./results/T2-judged/{MODEL}_{TASK}.json\", \"r\") as f:\n",
    "    judged = json.load(f)\n",
    "\n",
    "with open(f\"./data/test_data/{TASK}_test.json\", \"r\") as f:\n",
    "# with open(f\"./data/mmlu_pro/{TASK}_test.json\", \"r\") as f:\n",
    "# with open(f\"./data/shuffled/{TASK}.json\", \"r\")v as f:\n",
    "# data_dir = f\"./data/cognitive/{TASK}_test.json\"\n",
    "    original = json.load(f)\n",
    "\n",
    "pattern = r'\\(([A-Z])\\)\\s*([^()]+)'\n",
    "\n",
    "for x, (o, j) in enumerate(zip(original, judged)):\n",
    "    try:\n",
    "        j['answer'] = [a.replace(\"(\", \"\").replace(\")\", \"\") for a in j['answer']]\n",
    "        keep = list(set(filter(None, j['answer'])))\n",
    "        \n",
    "        processed_items = (item.replace(\"(\", \"\").replace(\")\", \"\") for item in j['answer'] if isinstance(item, str))\n",
    "        \n",
    "        keep = list(set(\n",
    "            item.upper().strip() for item in processed_items \n",
    "            if len(item) == 1\n",
    "        ))\n",
    "        \n",
    "        # Step 1: extract all (A)...(D) options\n",
    "        text = o['question']\n",
    "        options = dict(re.findall(pattern, text))\n",
    "    \n",
    "        # Step 2: keep only selected ones\n",
    "        filtered = [(k, options[k].strip()) for k in keep]\n",
    "    \n",
    "        # Step 3: extract the question part (everything before the first option)\n",
    "        question_part = re.split(pattern, text)[0].strip()\n",
    "    \n",
    "        # Step 4: rewrite with the kept options, renumbering them (A), (B), ...\n",
    "        new_options = [f\"({chr(65+i)}) {v}\" for i, (_, v) in enumerate(filtered)]\n",
    "        rewritten = question_part + \" \" + \" \".join(new_options)\n",
    "    \n",
    "        j['question'] = rewritten\n",
    "    \n",
    "        # Step 5: update gold_answer\n",
    "        if o['gold_answer'] in keep:\n",
    "            new_index = keep.index(o['gold_answer'])\n",
    "            updated_gold = chr(65 + new_index) \n",
    "        else:\n",
    "            updated_gold = \"N/A\"\n",
    "        j['gold_answer'] = updated_gold\n",
    "    except Exception as e:\n",
    "        print(x, e)\n",
    "        \n",
    "os.makedirs(\"./data/reformed/\", exist_ok=True)\n",
    "save_path = f\"./data/reformed/{MODEL}_{TASK}.json\"\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(judged, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 python evaluate2.py --task $TASK --n 100 --model $MODEL --data_dir $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae1d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python judge2.py --task $TASK --model $MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5177952",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./data/test_data/{TASK}_test.json\", \"r\") as f:\n",
    "    original = json.load(f)\n",
    "\n",
    "with open(f\"./results/T2-judged/{MODEL}_{TASK}.json\", \"r\") as f:\n",
    "    reformed = json.load(f)\n",
    "\n",
    "n_correct_1=0\n",
    "n_correct_2=0\n",
    "n_correct_3=0\n",
    "for x, (o, r) in enumerate(zip(original, reformed)):\n",
    "    r['answer'] = [a.replace(\"(\", \"\").replace(\")\", \"\") for a in r['answer']]\n",
    "    if o['gold_answer'] == r['answer'][0]:\n",
    "        n_correct_1+=1\n",
    "    \n",
    "    if r['answer'][0] == r['answer'][1] == o['gold_answer']:\n",
    "        n_correct_2+=1\n",
    "        n_correct_3+=1\n",
    "    else:\n",
    "        if o['gold_answer'] in r['answer'][0:2]:\n",
    "            n_correct_2+=1\n",
    "        if r['gold_answer'] == r['answer'][2]:\n",
    "            n_correct_3+=1\n",
    "\n",
    "print(\"Accuracy: \", n_correct_1/(x+1))\n",
    "print(\"Accuracy: \", n_correct_2/(x+1))\n",
    "print(\"Accuracy: \", n_correct_3/(x+1))\n",
    "print(f'{MODEL}_{TASK}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52bb40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
